library(kknn)
# Read in data
train <- vroom("train.csv")
test <- vroom("test.csv")
# Make target (response) a factor
train$target <- factor(train$target)
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors())
# Create K-nearest neighbors model
knn_mod <- nearest_neighbor(neighbors = tune()) |>
set_mode("classification") |>
set_engine("kknn")
# Create workflow
knn_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(knn_mod)
# Grid of values to tune over
tuning <- grid_regular(neighbors(range = c(1, 20)),
levels = 5)
# Folds for cross validation
folds <- vfold_cv(data = train, v = 5, repeats = 1)
# Run cross validation
cv_results <- knn_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
# Find best tuning parameters
best_tuning <- cv_results |>
select_best(metric = "mn_log_loss")
# Find best tuning parameters
best_tuning <- cv_results |>
select_best(metric = "accuracy")
# Finalize workflow and fit
final_wf <- knn_wf |>
finalize_workflow(best_tuning) |>
fit(data = train)
# Make predictions
knn_preds <- final_wf |>
predict(data = test, type = "prob")
# Make predictions
knn_preds <- final_wf |>
predict(new_data = test, type = "prob")
# Format for Kaggle submission
kaggle_sub <- knn_preds |>
bind_cols(test$id) |>
rename(id = ...10,
Class_1 = .pred_Class_1,
Class_2 = .pred_Class_2,
Class_3 = .pred_Class_3,
Class_4 = .pred_Class_4,
Class_5 = .pred_Class_5,
Class_6 = .pred_Class_6,
Class_7 = .pred_Class_7,
Class_8 = .pred_Class_8,
Class_9 = .pred_Class_9) |>
select(id, everything())
# Write out file
vroom_write(x = kaggle_sub, file = "submission3.csv", delim = ",")
library(tidymodels)
library(vroom)
library(embed)
library(kknn)
# Read in data
train <- vroom("train.csv")
test <- vroom("test.csv")
# Make target (response) a factor
train$target <- factor(train$target)
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors())
# Create K-nearest neighbors model
knn_mod <- nearest_neighbor(neighbors = tune()) |>
set_mode("classification") |>
set_engine("kknn")
# Create workflow
knn_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(knn_mod)
# Grid of values to tune over
tuning <- grid_regular(neighbors(range = c(1, 20)),
levels = 5)
# Folds for cross validation
folds <- vfold_cv(data = train, v = 5, repeats = 1)
# Run cross validation
cv_results <- knn_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy, mn_log_loss))
# Find best tuning parameters
best_tuning <- cv_results |>
select_best(metric = "mn_log_loss")
# Finalize workflow and fit
final_wf <- knn_wf |>
finalize_workflow(best_tuning) |>
fit(data = train)
# Make predictions
knn_preds <- final_wf |>
predict(new_data = test, type = "prob")
View(best_tuning)
# Format for Kaggle submission
kaggle_sub <- knn_preds |>
bind_cols(test$id) |>
rename(id = ...10,
Class_1 = .pred_Class_1,
Class_2 = .pred_Class_2,
Class_3 = .pred_Class_3,
Class_4 = .pred_Class_4,
Class_5 = .pred_Class_5,
Class_6 = .pred_Class_6,
Class_7 = .pred_Class_7,
Class_8 = .pred_Class_8,
Class_9 = .pred_Class_9) |>
select(id, everything())
# Write out file
vroom_write(x = kaggle_sub, file = "submission5.csv", delim = ",")
library(tidymodels)
library(vroom)
library(discrim)
library(embed)
library(lme4)
# Read in data
train <- vroom("train.csv")
test <- vroom("test.csv")
# Make target (response) a factor
train$target <- factor(train$target)
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_dummy(all_nominal_predictors())
# Create naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) |>
set_mode("classification") |>
set_engine("naivebayes")
# Create workflow
nb_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(nb_mod)
# Grid of values to tune over
tuning <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors())
# Create naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) |>
set_mode("classification") |>
set_engine("naivebayes")
# Create workflow
nb_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(nb_mod)
# Grid of values to tune over
tuning <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
test <- vroom("test.csv")
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors())
# Read in data
train <- vroom("train.csv")
test <- vroom("test.csv")
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors())
# Create naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) |>
set_mode("classification") |>
set_engine("naivebayes")
# Create workflow
nb_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(nb_mod)
# Grid of values to tune over
tuning <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors()) |>
step_smote(target)
# Create naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) |>
set_mode("classification") |>
set_engine("naivebayes")
# Create workflow
nb_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(nb_mod)
# Grid of values to tune over
tuning <- grid_regular(Laplace(),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
# Grid of values to tune over
tuning <- grid_regular(Laplace(range = c(0.01, 2)),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
library(tidymodels)
library(vroom)
library(discrim)
library(embed)
library(lme4)
# Read in data
train <- vroom("train.csv")
test <- vroom("test.csv")
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_smote(target)
# Create naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) |>
set_mode("classification") |>
set_engine("naivebayes")
# Create workflow
nb_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(nb_mod)
# Grid of values to tune over
tuning <- grid_regular(Laplace(range = c(0.01, 2)),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
library(themis)
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_smote(target)
# Create naive Bayes model
nb_mod <- naive_Bayes(Laplace = tune(),
smoothness = tune()) |>
set_mode("classification") |>
set_engine("naivebayes")
# Create workflow
nb_wf <- workflow() |>
add_recipe(otto_recipe) |>
add_model(nb_mod)
# Grid of values to tune over
tuning <- grid_regular(Laplace(range = c(0.01, 2)),
smoothness(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- nb_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy))
# Find best tuning parameters
best_tuning <- cv_results |>
select_best(metric = "roc_auc")
# Finalize workflow and fit
final_wf <- nb_wf |>
finalize_workflow(best_tuning) |>
fit(data = train)
# Make predictions
nb_preds <- final_wf |>
predict(new_data = test, type = "prob")
# Format for Kaggle submission
kaggle_sub <- nb_preds |>
bind_cols(test$id) |>
rename(id = ...10,
Class_1 = .pred_Class_1,
Class_2 = .pred_Class_2,
Class_3 = .pred_Class_3,
Class_4 = .pred_Class_4,
Class_5 = .pred_Class_5,
Class_6 = .pred_Class_6,
Class_7 = .pred_Class_7,
Class_8 = .pred_Class_8,
Class_9 = .pred_Class_9) |>
select(id, everything())
# Write out file
vroom_write(x = kaggle_sub, file = "submission.csv", delim = ",")
library(tidymodels)
library(vroom)
library(bonsai)
library(lightgbm)
# Read in data
train <- vroom("train.csv")
test <- vroom("test.csv")
# Make target (repsonse) a factor
train$target <- factor(train$target)
# Create recipe
otto_recipe <- recipe(target ~ ., data = train) |>
step_rm(id) |>
step_dummy(all_nominal_predictors())
# Create boosted model
boost_mod <- boost_tree(trees = 10,
tree_depth = 4,
learn_rate = 0.1) |>
set_engine("lightgbm") |>
set_mode("classification")
# Grid of values to tune over
tuning <- grid_regular(tree_depth(),
learn_rate(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- boost_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy, mn_log_loss))
# Create boosted model
boost_mod <- boost_tree(trees = 5,
tree_depth = 2,
learn_rate = 0.1) |>
set_engine("lightgbm") |>
set_mode("classification")
# Grid of values to tune over
tuning <- grid_regular(tree_depth(),
learn_rate(),
levels = 5)
# Split data for cross validation
folds <- vfold_cv(train, v = 5, repeats = 1)
# Run cross validation
cv_results <- boost_wf |>
tune_grid(resamples = folds,
grid = tuning,
metrics = metric_set(roc_auc, f_meas, accuracy, mn_log_loss))
# Load data
otto_test = vroom("test.csv")
otto_train = vroom("train.csv")
otto_train$target = factor(otto_train$target)
#### RECIPE ####
otto_recipe <- recipe(target~., data=otto_train) %>%
step_rm(id) %>%    # remove ID column
step_normalize(all_numeric_predictors())  # normalize all numeric predictors
#### LIGHT GBM ####
otto_lgbm <- boost_tree(tree_depth = 4,  # tune()
learn_rate = 0.1,  # tune()
trees = 1000) %>%
set_engine("lightgbm") %>%
set_mode("classification")
## CV
folds <- vfold_cv(otto_train, v=5, repeats=1)
CV_results_lgbm <- fit_resamples(otto_lgbm,
otto_recipe,
folds,
metrics = metric_set(mn_log_loss),
control = control_resamples(save_pred = T))
#### FINALIZE WF ####
lgbm_workflow <- workflow() %>%
add_recipe(otto_recipe) %>%
add_model(otto_lgbm) %>%
fit(data = otto_train)
#### MAKE PREDICTIONS ####
otto_preds <- lgbm_workflow %>%
predict(new_data = otto_test, type="prob")
## Format predictions for kaggle upload
otto_kaggle_submission <- otto_preds %>%
bind_cols(otto_test$id) %>%
rename("id" = "...10",
"Class_1"= ".pred_Class_1",
"Class_2"= ".pred_Class_2",
"Class_3"= ".pred_Class_3",
"Class_4"= ".pred_Class_4",
"Class_5"= ".pred_Class_5",
"Class_6"= ".pred_Class_6",
"Class_7"= ".pred_Class_7",
"Class_8"= ".pred_Class_8",
"Class_9"= ".pred_Class_9") %>%
select(id, everything())
## Write out file
vroom_write(x=otto_kaggle_submission, file="submission6.csv", delim=",")
# Load libraries
library(tidyverse)
library(tidymodels)
library(vroom)
library(lightgbm)
library(bonsai)
library(ranger)
# Read in data
otto_test = vroom("test.csv")
otto_train = vroom("train.csv")
otto_train$target = factor(otto_train$target)
# Create recipe
otto_recipe <- recipe(target~., data=otto_train) |>
step_rm(id) |>
step_normalize(all_numeric_predictors())
# Create boosted tree model (light gbm)
otto_lgbm <- boost_tree(tree_depth = tune(),
learn_rate = tune(),
trees = tune()) |>
set_engine("lightgbm") |>
set_mode("classification")
# Create workflow
lgbm_workflow <- workflow() |>
add_recipe(otto_recipe) |>
add_model(otto_lgbm) |>
fit(data = otto_train)
# Split data for cross validation
folds <- vfold_cv(otto_train, v=5, repeats=1)
# Create workflow
lgbm_workflow <- workflow() |>
add_recipe(otto_recipe) |>
add_model(otto_lgbm)
# Split data for cross validation
folds <- vfold_cv(otto_train, v = 5, repeats = 1)
# Grid of values to tune over
tuning_grid <- grid_regular(tree_depth(),
learn_rate(),
levels=5)
# Run cross validation
cv_results <- lgbm_workflow |>
tune_grid(resamples=folds,
grid=tuning_grid_lgbm,
metrics=metric_set(mn_log_loss))
# Run cross validation
cv_results <- lgbm_workflow |>
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(mn_log_loss))
# Grid of values to tune over
tuning_grid <- grid_regular(trees(),
tree_depth(),
learn_rate(),
levels=5)
# Run cross validation
cv_results <- lgbm_workflow |>
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(mn_log_loss))
# Create boosted tree model (light gbm)
otto_lgbm <- boost_tree(tree_depth = tune(),
learn_rate = tune(),
trees = 1000) |>
set_engine("lightgbm") |>
set_mode("classification")
# Create workflow
lgbm_workflow <- workflow() |>
add_recipe(otto_recipe) |>
add_model(otto_lgbm)
# Split data for cross validation
folds <- vfold_cv(otto_train, v = 5, repeats = 1)
# Grid of values to tune over
tuning_grid <- grid_regular(tree_depth(),
learn_rate(),
levels=5)
# Run cross validation
cv_results <- lgbm_workflow |>
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(mn_log_loss))
# Fit
otto_fit <- fit_resamples(otto_lgbm,
otto_recipe,
folds,
metrics = metric_set(mn_log_loss),
control = control_resamples(save_pred = TRUE))
library(tidymodels)
library(vroom)
library(bonsai)
library(lightgbm)
# Read in data
train <- vroom("train.csv")
View(train)
test <- vroom("test.csv")
